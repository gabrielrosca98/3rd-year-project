{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time \n",
    "import pwd\n",
    "import re\n",
    "import io\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from scipy import stats\n",
    "from scipy.stats import shapiro\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import sys\n",
    "import string\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "import math\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import warnings\n",
    "import boto3\n",
    "import s3fs\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from IPython.display import Markdown, display\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import FileUpload\n",
    "from ipywidgets import TwoByTwoLayout\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "from ipywidgets import VBox, HBox, Label, Box\n",
    "from ipywidgets import Button, Layout\n",
    "from pandas.errors import EmptyDataError \n",
    "#Print function\n",
    "def printmd(string):\n",
    "    display(Markdown(string))\n",
    "np.set_printoptions(threshold=100000)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def get_file_path(req_file_name):\n",
    "    for root, directory, file in os.walk(r'../'):\n",
    "        for name in file:\n",
    "            if name == req_file_name:\n",
    "                return os.path.abspath(os.path.join(root, name))\n",
    "            \n",
    "def clean_data(req_df):\n",
    "    req_df = req_df.dropna(axis = 1, thresh = req_df.shape[0] / 2)\n",
    "    req_df = req_df.dropna(axis = 0, thresh = req_df.shape[1] / 2)\n",
    "    req_df = req_df.reset_index(drop=True)\n",
    "    return req_df\n",
    "\n",
    "def read_data(req_upload):\n",
    "    file_name = next(iter(req_upload.value))\n",
    "    file_path = get_file_path(file_name)\n",
    "    content = upload.value[file_name]['content']\n",
    "    try:\n",
    "        data = pd.read_csv(file_path)          \n",
    "    except UnicodeDecodeError:\n",
    "        data = pd.read_csv(file_path, encoding ='latin-1')\n",
    "    data = clean_data(data)\n",
    "    st = os.stat(file_path)\n",
    "    return data, st, file_name, file_path\n",
    "\n",
    "#Set the stop words\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "#Initialise the words corpus\n",
    "words_corpora = set(nltk.corpus.words.words())\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "\n",
    "#Initialise the date words\n",
    "stop_words_time = ['date', 'time', 'year', 'hour', 'o\\'clock']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Administrative metadata\n",
    "\n",
    "##### Administrative metadata is information to help manage a resource, like resource type, permissions, and when and how it was created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def administrative_metadata(req_file_path, file_name, req_st):\n",
    "    #Check for access\n",
    "    read_access = os.access(file_path, os.R_OK)\n",
    "    write_access = os.access(file_path, os.W_OK)\n",
    "    execution_access = os.access(file_path, os.X_OK)\n",
    "    existance_file = os.access(file_path, os.F_OK)    \n",
    "    \n",
    "    #Create dataframe\n",
    "    administrative_data = pd.DataFrame(columns = ['file_name', 'file_read_access', 'file_write_access', 'file_execution_access', 'file_existance', 'file_creation_date'])\n",
    "\n",
    "    #Set values\n",
    "    administrative_data.loc[0, 'file_name'] = file_name\n",
    "    administrative_data.loc[0, 'file_read_access'] = read_access\n",
    "    administrative_data.loc[0, 'file_write_access'] = write_access\n",
    "    administrative_data.loc[0, 'file_execution_access'] = execution_access\n",
    "    administrative_data.loc[0, 'file_existance'] = existance_file\n",
    "    administrative_data.loc[0, 'file_creation_date'] = time.ctime(req_st.st_ctime)\n",
    "    \n",
    "    return administrative_data\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Descriptive metadata\n",
    "\n",
    "##### Descriptive metadata is descriptive information about a resource. It is used for discovery and identification. It includes elements such as title, abstract, author, and keywords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter function to filter out time columns and columns that provide little information\n",
    "def filter_columns(req_string): \n",
    "    pattern  = re.compile(r'' + \"|\".join(stop_words_time), re.IGNORECASE)\n",
    "    is_string_column = lambda x : data[x].dtypes == 'O' or data[x].dtypes == 'S'\n",
    "    return not bool(pattern.search(req_string)) and is_string_column(req_string)\n",
    "\n",
    "#Check if a column is about time/dates\n",
    "def is_time_column(req_string):\n",
    "    pattern  = re.compile(r'' + \"|\".join(stop_words_time), re.IGNORECASE)\n",
    "    return bool(pattern.search(req_string))\n",
    "\n",
    "#Check if the words in the columns appear in the English corpus \n",
    "def is_meaningful_word(req_string):\n",
    "    is_meaningful = pd.Series(list(map(lambda x: \"True\" if ((x != 'nan' and pd.isnull(x) != True) and str(x).lower().translate(str.maketrans('', '', string.punctuation)) in words_corpora) else \"False\", \" \".join(map(str,req_string)).split()))).value_counts()\n",
    "    return 'True' in is_meaningful.index.values and is_meaningful['True'] > 0\n",
    "\n",
    "def data_keywords(req_data):\n",
    "    column_names = req_data.columns\n",
    "    df_nlp = pd.DataFrame()\n",
    "    keywords = list()\n",
    "    for column in filter(filter_columns, column_names):\n",
    "        #Tokenize and lower case\n",
    "        df_nlp[column] = req_data[column].apply(lambda x : str(x).lower().split(\" \"))\n",
    "        vectorizer = TfidfVectorizer(stop_words=\"english\", lowercase = False, ngram_range = (1,2))\n",
    "        try:\n",
    "            #Create the tfidf matrix \n",
    "            tfidf_matrix = vectorizer.fit_transform(df_nlp[column].astype(str).tolist())\n",
    "            \n",
    "            #Get the tokens\n",
    "            feature_names = vectorizer.get_feature_names()\n",
    "            \n",
    "            #Get the words that appear\n",
    "            feature_indexes = tfidf_matrix[:,:].nonzero()[1]\n",
    "            \n",
    "            #Create a DataFrame to store the tokens with their tfidf\n",
    "            data_tfidf = pd.DataFrame(tfidf_matrix.toarray(), columns = vectorizer.get_feature_names())\n",
    "            \n",
    "            #Sort the values in descending order\n",
    "            data_tfidf = data_tfidf.apply(lambda x : x.mean()).sort_values(ascending=False)            \n",
    "            if len(data_tfidf) >= 3:\n",
    "                #Test the \n",
    "                stat, p = shapiro(data_tfidf)\n",
    "                alpha = 0.05\n",
    "                if p <= alpha and is_meaningful_word(req_data[column]):\n",
    "                    keywords.append((data_tfidf[data_tfidf > data_tfidf.describe()['max'] - data_tfidf.describe()['std']].index.tolist()))\n",
    "        except ValueError:\n",
    "            pass\n",
    "    for column in column_names:\n",
    "        if is_time_column(column):\n",
    "            keywords.append([\"time analysis\"])\n",
    "            break;\n",
    "    return keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def descriptive_metadata(req_file_path, req_data, req_file_name):\n",
    "    st = os.stat(req_file_path)\n",
    "    descriptive_data = pd.DataFrame(columns = ['file_name', 'file_author'])\n",
    "    author_name = pwd.getpwuid(st.st_uid).pw_gecos\n",
    "    descriptive_data.loc[0, 'file_name'] = req_file_name\n",
    "    descriptive_data.loc[0, 'file_author'] = author_name\n",
    "    keywords = data_keywords(req_data)\n",
    "    descriptive_data.loc[0, 'keywords'] = \"\\n\".join(map(str, keywords))  \n",
    "    return descriptive_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structural\n",
    "\n",
    "###### Structural metadata is metadata about containers of data and indicates how compound objects are put together, for example, how pages are ordered to form chapters. It describes the types, versions, relationships and other characteristics of digital materials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def structural_metadata(req_file_name, req_data):\n",
    "    #Create a DataFrame to store the structural metadata\n",
    "    structural_data = pd.DataFrame(columns = ['column_types'])\n",
    "    col_type = list()\n",
    "    for column in req_data.columns:\n",
    "        col_type.append(type(req_data[column][0]))\n",
    "    col_type = set(col_type)\n",
    "    \n",
    "    #Get the information\n",
    "    structural_data.loc[0, 'file_name'] = req_file_name\n",
    "    structural_data.loc[0, 'column_types'] = \",\".join(map(str,col_type))\n",
    "    structural_data.loc[0, 'column_numbers'] = len(data.columns)\n",
    "    structural_data.loc[0, 'entries'] = data.shape[0]\n",
    "    return structural_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_meta_data(req_file_path, req_file_name, req_data, req_st):\n",
    "    #Administrative metadata\n",
    "    administrative_mt = administrative_metadata(req_file_path, req_file_name, req_st)\n",
    "    \n",
    "    #Descriptive metadata\n",
    "    descriptive_mt = descriptive_metadata(req_file_path, req_data, req_file_name)\n",
    "    \n",
    "    #Structural metadata\n",
    "    structural_mt = structural_metadata(req_file_name ,req_data)\n",
    "    \n",
    "    #Create metadata dataframe\n",
    "    mt_data = pd.DataFrame()\n",
    "    mt_data = pd.merge(administrative_mt, descriptive_mt, on=['file_name'])\n",
    "    mt_data = pd.merge(mt_data, structural_mt, on=['file_name'])\n",
    "    return mt_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choose the file to get the metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local metadata file\n",
    "\n",
    "Try to get a metadata file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    meta_data = pd.read_csv('../data_meta.csv')\n",
    "    index_file = meta_data.shape[0] - 1\n",
    "except OSError:\n",
    "    meta_data = pd.DataFrame(columns=['file_name', 'file_read_access', 'file_write_access',\n",
    "       'file_execution_access', 'file_existance', 'file_creation_date',\n",
    "       'file_author', 'keywords', 'column_types', 'column_numbers', 'entries'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single file - local "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select the file using File explorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upload = FileUpload(accept='.csv')\n",
    "upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data, st, file_name, file_path = read_data(upload)\n",
    "data, st, file_name, file_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the metadata for the selected file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mt_data_current_file = get_meta_data(file_path, file_name, data, st)\n",
    "meta_data = meta_data.append(mt_data_current_file)\n",
    "meta_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple files - one single local metadata file\n",
    "\n",
    "\n",
    "For multiple files to get the metadata file. Change the path_data variable with the desired folder in order to select the required files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "path_data = \"..\"\n",
    "folders_files = os.walk(\"..\")\n",
    "# if \".DS_Store\" in folders:\n",
    "#     folders.remove(\".DS_Store\")\n",
    "folders = []\n",
    "for x in folders_files:\n",
    "    folders.append(x[0])\n",
    "for current_folder in tqdm(folders, desc = \"Folders\", leave = True):\n",
    "    for file_name in os.listdir(current_folder):\n",
    "        if file_name.endswith(\".csv\") == True:\n",
    "            file_path = current_folder + \"/\" + file_name\n",
    "            st = os.stat(file_path)\n",
    "            try:\n",
    "                data = pd.read_csv(file_path)          \n",
    "            except UnicodeDecodeError:\n",
    "                data = pd.read_csv(file_path, encoding ='latin-1')\n",
    "            except EmptyDataError:\n",
    "                pass\n",
    "            data = clean_data(data)\n",
    "            if data.empty == False:\n",
    "                mt_data_current_file = get_meta_data(file_path, file_name, data, st)\n",
    "                meta_data = meta_data.append(mt_data_current_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write the metadata to the local file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_data = meta_data.reset_index()\n",
    "meta_data = meta_data.drop([\"index\"],axis=1)\n",
    "meta_data.to_csv('../data_meta.csv', index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write the metadata file to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.resource('s3')\n",
    "s3_client = boto3.client('s3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List all buckets available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_all_buckets():\n",
    "    try:\n",
    "        bucket_list = [bucket.name for bucket in s3.buckets.all()]\n",
    "        return bucket_list\n",
    "    except Exception as e:\n",
    "        print(\"Unexpected error: %s\", e)\n",
    "\n",
    "print('Available buckets: ' + str(list_all_buckets()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List all files in a bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def list_all_buckets(req_bucket_name = \"\", req_bucket = None): \n",
    "    try:\n",
    "        folders_bucket = list()\n",
    "        objects_list = list()\n",
    "        if req_bucket_name != \"\":\n",
    "            my_bucket = s3.Bucket(req_bucket_name)\n",
    "        elif req_bucket != None:\n",
    "            my_bucket = req_bucket\n",
    "        else:\n",
    "            print(\"Please provide an argument\")\n",
    "            return None\n",
    "        for object in my_bucket.objects.all():\n",
    "            if(re.match(r\".*\\..*\", object.key)):    \n",
    "                object_name = object.key[object.key.rfind(\"/\") + 1:]\n",
    "                objects_list.append(object_name)\n",
    "            else: \n",
    "                folders_bucket.append(object.key)\n",
    "        return objects_list, folders_bucket\n",
    "    except Exception as e:\n",
    "        print(\"Unexpected error: %s\" % e)\n",
    "        \n",
    "mybucket = s3.Bucket(\"uom.bioinformatics\") \n",
    "bucket_files, bucket_folders = list_all_buckets(req_bucket = mybucket)\n",
    "\n",
    "printmd(\"**Files in bucket \" + mybucket.name + \":** \\n\")\n",
    "for value in bucket_files:\n",
    "    print(value)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List all folders in a bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printmd(\"**Folders in bucket \" + mybucket.name + \":**\\n\")\n",
    "for value in bucket_folders:\n",
    "    print(value)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write single metadata file to a bucket folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client.upload_file(\"../data_meta.csv\", \"uom.bioinformatics\", \"data_meta.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
